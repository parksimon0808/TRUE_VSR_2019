{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURS_PER_DAY = 20\n",
    "DAYS_PER_WEEK = 7\n",
    "DAYS_PER_YEAR = 365\n",
    "\n",
    "# process the raw input data and generate input, output pair for the LSTM model\n",
    "#\n",
    "# input: [ [x[t - hours], ..., x[t - 1hour]], \n",
    "#          [x[t - days],  ..., x[t - 1day]], \n",
    "#          [x[t - weeks], ..., x[t - 1week]],\n",
    "#          [x[t - years], ..., x[t - 1year]]  ]\n",
    "# \n",
    "# output: x[t]\n",
    "def create_dataset_years(data, hours=1, days=1, weeks=1, years=1):\n",
    "    num_data = len(data) - HOURS_PER_DAY * DAYS_PER_YEAR * years\n",
    "    x_arr, y_arr = np.zeros((num_data, 4, max(hours, days, weeks, years))), np.zeros((num_data,))\n",
    "    \n",
    "    for i in range(num_data):\n",
    "        index = i\n",
    "        \n",
    "        for j in range(years):\n",
    "            x_arr[i, 3, j] = data[index]\n",
    "            index += HOURS_PER_DAY * DAYS_PER_YEAR\n",
    "            \n",
    "        index -= HOURS_PER_DAY * DAYS_PER_WEEK * weeks\n",
    "        \n",
    "        for j in range(weeks):\n",
    "            x_arr[i, 2, j] = data[index]\n",
    "            index += HOURS_PER_DAY * DAYS_PER_WEEK\n",
    "            \n",
    "        index -= HOURS_PER_DAY * days\n",
    "        \n",
    "        for j in range(days):\n",
    "            x_arr[i, 1, j] = data[index]\n",
    "            index += HOURS_PER_DAY\n",
    "        \n",
    "        x_arr[i, 0, 0:hours] = data[(index-hours):index]\n",
    "        y_arr[i] = data[index]\n",
    "\n",
    "    return x_arr, y_arr\n",
    "\n",
    "# create an LSTM model to train with\n",
    "def create_model():\n",
    "    model = Sequential()  \n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(Dropout(0.2)) \n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "# run the LSTM model with the given data\n",
    "def run_model(data):\n",
    "    hours = 3\n",
    "    days = 4\n",
    "    weeks = 5\n",
    "    years = 5\n",
    "    batch_size = 256\n",
    "    \n",
    "    # create model\n",
    "    model = create_model()\n",
    "    adam = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "    \n",
    "    # prepare data\n",
    "    x_data, y_data = create_dataset_years(data, hours, days, weeks, years)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, shuffle=False)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # run model\n",
    "    history = model.fit(x_train, y_train, epochs=200, batch_size=batch_size, validation_data=(x_val, y_val))\n",
    "    score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "    # predict future values\n",
    "    predictions = np.concatenate(model.predict(x_test, batch_size))\n",
    "    #plt.plot(predictions, (predictions - y_test), 'rx')\n",
    "    \n",
    "    # evaluate model\n",
    "    SMAPE = np.mean(abs(predictions - y_test) / (abs(predictions) + abs(y_test)))\n",
    "    RMSE = np.sqrt(np.mean((predictions - y_test)**2))\n",
    "    \n",
    "    return SMAPE, RMSE, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22220 samples, validate on 5556 samples\n",
      "Epoch 1/200\n",
      "22220/22220 [==============================] - 7s 304us/step - loss: 0.0099 - val_loss: 0.0020\n",
      "Epoch 2/200\n",
      "22220/22220 [==============================] - 6s 286us/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 3/200\n",
      "22220/22220 [==============================] - 3s 149us/step - loss: 0.0025 - val_loss: 0.0015\n",
      "Epoch 4/200\n",
      "22220/22220 [==============================] - 3s 151us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 5/200\n",
      "22220/22220 [==============================] - 3s 147us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 6/200\n",
      "22220/22220 [==============================] - 3s 145us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 7/200\n",
      "22220/22220 [==============================] - 4s 160us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 8/200\n",
      "22220/22220 [==============================] - 4s 181us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 9/200\n",
      "22220/22220 [==============================] - 5s 207us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 10/200\n",
      "22220/22220 [==============================] - 4s 159us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 11/200\n",
      "22220/22220 [==============================] - 4s 189us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 12/200\n",
      "22220/22220 [==============================] - 4s 171us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 13/200\n",
      "22220/22220 [==============================] - 4s 178us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 14/200\n",
      "22220/22220 [==============================] - 4s 163us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 15/200\n",
      "22220/22220 [==============================] - 4s 170us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 16/200\n",
      "22220/22220 [==============================] - 4s 184us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 17/200\n",
      "22220/22220 [==============================] - 4s 166us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 18/200\n",
      "22220/22220 [==============================] - 4s 166us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 19/200\n",
      "22220/22220 [==============================] - 3s 153us/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 20/200\n",
      "22220/22220 [==============================] - 4s 170us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 21/200\n",
      "22220/22220 [==============================] - 4s 161us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 22/200\n",
      "22220/22220 [==============================] - 3s 153us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 23/200\n",
      "22220/22220 [==============================] - 3s 157us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 24/200\n",
      "22220/22220 [==============================] - 4s 177us/step - loss: 0.0018 - val_loss: 0.0010\n",
      "Epoch 25/200\n",
      "22220/22220 [==============================] - 4s 162us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 26/200\n",
      "22220/22220 [==============================] - 4s 159us/step - loss: 0.0017 - val_loss: 0.0010\n",
      "Epoch 27/200\n",
      "22220/22220 [==============================] - 4s 165us/step - loss: 0.0017 - val_loss: 9.6232e-04\n",
      "Epoch 28/200\n",
      "22220/22220 [==============================] - 3s 151us/step - loss: 0.0017 - val_loss: 9.4501e-04\n",
      "Epoch 29/200\n",
      "22220/22220 [==============================] - 3s 150us/step - loss: 0.0016 - val_loss: 9.1929e-04\n",
      "Epoch 30/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 0.0016 - val_loss: 9.5902e-04\n",
      "Epoch 31/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 0.0016 - val_loss: 8.6456e-04\n",
      "Epoch 32/200\n",
      "22220/22220 [==============================] - 3s 150us/step - loss: 0.0016 - val_loss: 9.3275e-04\n",
      "Epoch 33/200\n",
      "22220/22220 [==============================] - 4s 166us/step - loss: 0.0015 - val_loss: 8.7486e-04\n",
      "Epoch 34/200\n",
      "22220/22220 [==============================] - 4s 167us/step - loss: 0.0015 - val_loss: 8.4797e-04\n",
      "Epoch 35/200\n",
      "22220/22220 [==============================] - 4s 179us/step - loss: 0.0015 - val_loss: 8.5442e-04\n",
      "Epoch 36/200\n",
      "22220/22220 [==============================] - 4s 175us/step - loss: 0.0015 - val_loss: 9.0387e-04\n",
      "Epoch 37/200\n",
      "22220/22220 [==============================] - 4s 170us/step - loss: 0.0015 - val_loss: 8.3759e-04\n",
      "Epoch 38/200\n",
      "22220/22220 [==============================] - 4s 170us/step - loss: 0.0015 - val_loss: 8.2928e-04\n",
      "Epoch 39/200\n",
      "22220/22220 [==============================] - 4s 177us/step - loss: 0.0015 - val_loss: 8.3851e-04\n",
      "Epoch 40/200\n",
      "22220/22220 [==============================] - 4s 191us/step - loss: 0.0015 - val_loss: 9.4373e-04\n",
      "Epoch 41/200\n",
      "22220/22220 [==============================] - 4s 172us/step - loss: 0.0015 - val_loss: 8.3447e-04\n",
      "Epoch 42/200\n",
      "22220/22220 [==============================] - 4s 181us/step - loss: 0.0014 - val_loss: 8.1830e-04\n",
      "Epoch 43/200\n",
      "22220/22220 [==============================] - 4s 174us/step - loss: 0.0015 - val_loss: 8.8801e-04\n",
      "Epoch 44/200\n",
      "22220/22220 [==============================] - 4s 178us/step - loss: 0.0015 - val_loss: 8.7756e-04\n",
      "Epoch 45/200\n",
      "22220/22220 [==============================] - 4s 171us/step - loss: 0.0015 - val_loss: 8.5159e-04\n",
      "Epoch 46/200\n",
      "22220/22220 [==============================] - 4s 189us/step - loss: 0.0014 - val_loss: 8.6332e-04\n",
      "Epoch 47/200\n",
      "22220/22220 [==============================] - 4s 197us/step - loss: 0.0014 - val_loss: 8.8503e-04\n",
      "Epoch 48/200\n",
      "22220/22220 [==============================] - 4s 169us/step - loss: 0.0014 - val_loss: 8.3595e-04\n",
      "Epoch 49/200\n",
      "22220/22220 [==============================] - 4s 160us/step - loss: 0.0014 - val_loss: 8.8813e-04\n",
      "Epoch 50/200\n",
      "22220/22220 [==============================] - 4s 168us/step - loss: 0.0014 - val_loss: 8.1153e-04\n",
      "Epoch 51/200\n",
      "22220/22220 [==============================] - 3s 151us/step - loss: 0.0014 - val_loss: 7.9488e-04\n",
      "Epoch 52/200\n",
      "22220/22220 [==============================] - 3s 147us/step - loss: 0.0014 - val_loss: 8.6044e-04\n",
      "Epoch 53/200\n",
      "22220/22220 [==============================] - 3s 151us/step - loss: 0.0014 - val_loss: 8.3261e-04\n",
      "Epoch 54/200\n",
      "22220/22220 [==============================] - 4s 195us/step - loss: 0.0014 - val_loss: 7.8427e-04\n",
      "Epoch 55/200\n",
      "22220/22220 [==============================] - 4s 196us/step - loss: 0.0014 - val_loss: 8.8076e-04\n",
      "Epoch 56/200\n",
      "22220/22220 [==============================] - 4s 165us/step - loss: 0.0014 - val_loss: 8.7435e-04\n",
      "Epoch 57/200\n",
      "22220/22220 [==============================] - 3s 157us/step - loss: 0.0014 - val_loss: 8.3474e-04\n",
      "Epoch 58/200\n",
      "22220/22220 [==============================] - 4s 171us/step - loss: 0.0014 - val_loss: 8.1806e-04\n",
      "Epoch 59/200\n",
      "22220/22220 [==============================] - 4s 164us/step - loss: 0.0014 - val_loss: 8.1272e-04\n",
      "Epoch 60/200\n",
      "22220/22220 [==============================] - 4s 164us/step - loss: 0.0014 - val_loss: 8.8058e-04\n",
      "Epoch 61/200\n",
      "22220/22220 [==============================] - 3s 155us/step - loss: 0.0013 - val_loss: 8.4508e-04\n",
      "Epoch 62/200\n",
      "22220/22220 [==============================] - 4s 158us/step - loss: 0.0013 - val_loss: 8.1408e-04\n",
      "Epoch 63/200\n",
      "22220/22220 [==============================] - 4s 160us/step - loss: 0.0013 - val_loss: 8.7864e-04\n",
      "Epoch 64/200\n",
      "22220/22220 [==============================] - 4s 168us/step - loss: 0.0013 - val_loss: 7.8153e-04\n",
      "Epoch 65/200\n",
      "22220/22220 [==============================] - 4s 161us/step - loss: 0.0013 - val_loss: 7.6842e-04\n",
      "Epoch 66/200\n",
      "22220/22220 [==============================] - 4s 170us/step - loss: 0.0013 - val_loss: 7.4484e-04\n",
      "Epoch 67/200\n",
      "22220/22220 [==============================] - 3s 156us/step - loss: 0.0013 - val_loss: 7.9566e-04\n",
      "Epoch 68/200\n",
      "22220/22220 [==============================] - 3s 150us/step - loss: 0.0013 - val_loss: 7.8479e-04\n",
      "Epoch 69/200\n",
      "22220/22220 [==============================] - 3s 150us/step - loss: 0.0013 - val_loss: 7.6744e-04\n",
      "Epoch 70/200\n",
      "22220/22220 [==============================] - 3s 147us/step - loss: 0.0013 - val_loss: 7.8194e-04\n",
      "Epoch 71/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 0.0013 - val_loss: 7.2469e-04\n",
      "Epoch 72/200\n",
      "22220/22220 [==============================] - 3s 152us/step - loss: 0.0013 - val_loss: 7.1870e-04\n",
      "Epoch 73/200\n",
      "22220/22220 [==============================] - 4s 170us/step - loss: 0.0013 - val_loss: 7.1209e-04\n",
      "Epoch 74/200\n",
      "22220/22220 [==============================] - 4s 180us/step - loss: 0.0012 - val_loss: 6.9921e-04\n",
      "Epoch 75/200\n",
      "22220/22220 [==============================] - 4s 171us/step - loss: 0.0012 - val_loss: 7.0610e-04\n",
      "Epoch 76/200\n",
      "22220/22220 [==============================] - 4s 189us/step - loss: 0.0013 - val_loss: 6.8736e-04\n",
      "Epoch 77/200\n",
      "22220/22220 [==============================] - 5s 207us/step - loss: 0.0012 - val_loss: 8.0785e-04\n",
      "Epoch 78/200\n",
      "22220/22220 [==============================] - 6s 249us/step - loss: 0.0012 - val_loss: 7.0107e-04\n",
      "Epoch 79/200\n",
      "22220/22220 [==============================] - 4s 189us/step - loss: 0.0012 - val_loss: 7.0432e-04\n",
      "Epoch 80/200\n",
      "22220/22220 [==============================] - 4s 179us/step - loss: 0.0012 - val_loss: 6.7302e-04\n",
      "Epoch 81/200\n",
      "22220/22220 [==============================] - 4s 190us/step - loss: 0.0012 - val_loss: 6.6810e-04\n",
      "Epoch 82/200\n",
      "22220/22220 [==============================] - 4s 170us/step - loss: 0.0012 - val_loss: 7.4017e-04\n",
      "Epoch 83/200\n",
      "22220/22220 [==============================] - 3s 156us/step - loss: 0.0012 - val_loss: 6.7426e-04\n",
      "Epoch 84/200\n",
      "22220/22220 [==============================] - 3s 157us/step - loss: 0.0012 - val_loss: 6.3612e-04\n",
      "Epoch 85/200\n",
      "22220/22220 [==============================] - 3s 149us/step - loss: 0.0012 - val_loss: 6.7609e-04\n",
      "Epoch 86/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 0.0012 - val_loss: 6.2161e-04\n",
      "Epoch 87/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 0.0012 - val_loss: 6.6662e-04\n",
      "Epoch 88/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 0.0011 - val_loss: 6.4315e-04\n",
      "Epoch 89/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 0.0011 - val_loss: 7.7844e-04\n",
      "Epoch 90/200\n",
      "22220/22220 [==============================] - 3s 150us/step - loss: 0.0011 - val_loss: 6.0800e-04\n",
      "Epoch 91/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 0.0011 - val_loss: 6.5292e-04\n",
      "Epoch 92/200\n",
      "22220/22220 [==============================] - 3s 150us/step - loss: 0.0011 - val_loss: 5.9629e-04\n",
      "Epoch 93/200\n",
      "22220/22220 [==============================] - 3s 147us/step - loss: 0.0011 - val_loss: 6.2568e-04\n",
      "Epoch 94/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 0.0011 - val_loss: 6.1417e-04\n",
      "Epoch 95/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 0.0011 - val_loss: 6.3368e-04\n",
      "Epoch 96/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 0.0010 - val_loss: 5.9477e-04\n",
      "Epoch 97/200\n",
      "22220/22220 [==============================] - 3s 147us/step - loss: 0.0011 - val_loss: 6.4936e-04\n",
      "Epoch 98/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 0.0011 - val_loss: 6.4346e-04\n",
      "Epoch 99/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 0.0010 - val_loss: 5.8481e-04\n",
      "Epoch 100/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 0.0010 - val_loss: 5.8881e-04\n",
      "Epoch 101/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 0.0010 - val_loss: 5.5361e-04\n",
      "Epoch 102/200\n",
      "22220/22220 [==============================] - 3s 145us/step - loss: 0.0010 - val_loss: 6.1633e-04\n",
      "Epoch 103/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 0.0010 - val_loss: 5.5590e-04\n",
      "Epoch 104/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 9.9855e-04 - val_loss: 5.5867e-04\n",
      "Epoch 105/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 0.0010 - val_loss: 5.7547e-04\n",
      "Epoch 106/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 0.0010 - val_loss: 5.3993e-04\n",
      "Epoch 107/200\n",
      "22220/22220 [==============================] - 3s 147us/step - loss: 9.6067e-04 - val_loss: 5.1968e-04\n",
      "Epoch 108/200\n",
      "22220/22220 [==============================] - 3s 149us/step - loss: 9.6521e-04 - val_loss: 5.2793e-04\n",
      "Epoch 109/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 9.6152e-04 - val_loss: 5.5613e-04\n",
      "Epoch 110/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 9.3375e-04 - val_loss: 5.8179e-04\n",
      "Epoch 111/200\n",
      "22220/22220 [==============================] - 3s 148us/step - loss: 9.3639e-04 - val_loss: 5.6391e-04\n",
      "Epoch 112/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 9.5364e-04 - val_loss: 6.3278e-04\n",
      "Epoch 113/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 9.4596e-04 - val_loss: 5.1409e-04\n",
      "Epoch 114/200\n",
      "22220/22220 [==============================] - 3s 151us/step - loss: 9.4893e-04 - val_loss: 5.5838e-04\n",
      "Epoch 115/200\n",
      "22220/22220 [==============================] - 3s 149us/step - loss: 9.3555e-04 - val_loss: 5.4882e-04\n",
      "Epoch 116/200\n",
      "22220/22220 [==============================] - 3s 149us/step - loss: 9.3097e-04 - val_loss: 5.3698e-04\n",
      "Epoch 117/200\n",
      "22220/22220 [==============================] - 3s 156us/step - loss: 9.0920e-04 - val_loss: 5.0824e-04\n",
      "Epoch 118/200\n",
      "22220/22220 [==============================] - 3s 152us/step - loss: 9.2472e-04 - val_loss: 5.2892e-04\n",
      "Epoch 119/200\n",
      "22220/22220 [==============================] - 3s 151us/step - loss: 9.0332e-04 - val_loss: 4.8074e-04\n",
      "Epoch 120/200\n",
      "22220/22220 [==============================] - 3s 150us/step - loss: 9.0812e-04 - val_loss: 5.5988e-04\n",
      "Epoch 121/200\n",
      "22220/22220 [==============================] - 3s 152us/step - loss: 9.0789e-04 - val_loss: 4.7720e-04\n",
      "Epoch 122/200\n",
      "22220/22220 [==============================] - 4s 169us/step - loss: 9.3456e-04 - val_loss: 5.1791e-04\n",
      "Epoch 123/200\n",
      "22220/22220 [==============================] - 4s 158us/step - loss: 8.9954e-04 - val_loss: 4.9639e-04\n",
      "Epoch 124/200\n",
      "22220/22220 [==============================] - 4s 175us/step - loss: 8.9230e-04 - val_loss: 5.6001e-04\n",
      "Epoch 125/200\n",
      "22220/22220 [==============================] - 4s 186us/step - loss: 8.8718e-04 - val_loss: 4.9361e-04\n",
      "Epoch 126/200\n",
      "22220/22220 [==============================] - 4s 158us/step - loss: 8.7466e-04 - val_loss: 5.0434e-04\n",
      "Epoch 127/200\n",
      "22220/22220 [==============================] - 4s 163us/step - loss: 8.6179e-04 - val_loss: 6.2053e-04\n",
      "Epoch 128/200\n",
      "22220/22220 [==============================] - 4s 158us/step - loss: 8.7216e-04 - val_loss: 4.6830e-04\n",
      "Epoch 129/200\n",
      "22220/22220 [==============================] - 3s 151us/step - loss: 8.5010e-04 - val_loss: 4.9697e-04\n",
      "Epoch 130/200\n",
      "22220/22220 [==============================] - 4s 161us/step - loss: 8.2714e-04 - val_loss: 4.9338e-04\n",
      "Epoch 131/200\n",
      "22220/22220 [==============================] - 4s 174us/step - loss: 8.3782e-04 - val_loss: 4.9619e-04\n",
      "Epoch 132/200\n",
      "22220/22220 [==============================] - 3s 153us/step - loss: 8.7958e-04 - val_loss: 5.1768e-04\n",
      "Epoch 133/200\n",
      "22220/22220 [==============================] - 4s 161us/step - loss: 8.6016e-04 - val_loss: 5.1651e-04\n",
      "Epoch 134/200\n",
      "22220/22220 [==============================] - 3s 152us/step - loss: 8.5277e-04 - val_loss: 5.3903e-04\n",
      "Epoch 135/200\n",
      "22220/22220 [==============================] - 3s 149us/step - loss: 8.4872e-04 - val_loss: 4.8177e-04\n",
      "Epoch 136/200\n",
      "22220/22220 [==============================] - 3s 156us/step - loss: 8.2300e-04 - val_loss: 4.8508e-04\n",
      "Epoch 137/200\n",
      "22220/22220 [==============================] - 4s 172us/step - loss: 8.3815e-04 - val_loss: 5.3163e-04\n",
      "Epoch 138/200\n",
      "22220/22220 [==============================] - 4s 160us/step - loss: 8.0701e-04 - val_loss: 5.0390e-04\n",
      "Epoch 139/200\n",
      "22220/22220 [==============================] - 3s 152us/step - loss: 7.9674e-04 - val_loss: 5.3602e-04\n",
      "Epoch 140/200\n",
      "22220/22220 [==============================] - 4s 168us/step - loss: 7.9961e-04 - val_loss: 5.4415e-04\n",
      "Epoch 141/200\n",
      "22220/22220 [==============================] - 4s 171us/step - loss: 8.1899e-04 - val_loss: 5.1514e-04\n",
      "Epoch 142/200\n",
      "22220/22220 [==============================] - 4s 160us/step - loss: 8.4709e-04 - val_loss: 4.9111e-04\n",
      "Epoch 143/200\n",
      "22220/22220 [==============================] - 3s 151us/step - loss: 7.9102e-04 - val_loss: 4.9806e-04\n",
      "Epoch 144/200\n",
      "22220/22220 [==============================] - 3s 147us/step - loss: 7.6523e-04 - val_loss: 4.5856e-04\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22220/22220 [==============================] - 3s 154us/step - loss: 7.9233e-04 - val_loss: 4.9641e-04\n",
      "Epoch 146/200\n",
      "22220/22220 [==============================] - 4s 170us/step - loss: 7.8995e-04 - val_loss: 4.7693e-04\n",
      "Epoch 147/200\n",
      "22220/22220 [==============================] - 4s 186us/step - loss: 7.6825e-04 - val_loss: 4.9155e-04\n",
      "Epoch 148/200\n",
      "22220/22220 [==============================] - 4s 172us/step - loss: 7.7835e-04 - val_loss: 4.6282e-04\n",
      "Epoch 149/200\n",
      "22220/22220 [==============================] - 4s 169us/step - loss: 7.9973e-04 - val_loss: 4.8783e-04\n",
      "Epoch 150/200\n",
      "22220/22220 [==============================] - 4s 164us/step - loss: 7.9223e-04 - val_loss: 4.7120e-04\n",
      "Epoch 151/200\n",
      "22220/22220 [==============================] - 3s 154us/step - loss: 7.8235e-04 - val_loss: 4.6071e-04\n",
      "Epoch 152/200\n",
      "22220/22220 [==============================] - 3s 154us/step - loss: 7.6953e-04 - val_loss: 5.0642e-04\n",
      "Epoch 153/200\n",
      "22220/22220 [==============================] - 3s 152us/step - loss: 7.6641e-04 - val_loss: 5.1242e-04\n",
      "Epoch 154/200\n",
      "22220/22220 [==============================] - 3s 154us/step - loss: 7.5511e-04 - val_loss: 4.9401e-04\n",
      "Epoch 155/200\n",
      "22220/22220 [==============================] - 4s 169us/step - loss: 7.6904e-04 - val_loss: 4.9764e-04\n",
      "Epoch 156/200\n",
      "22220/22220 [==============================] - 6s 291us/step - loss: 7.9112e-04 - val_loss: 4.7224e-04\n",
      "Epoch 157/200\n",
      "22220/22220 [==============================] - 5s 218us/step - loss: 7.7071e-04 - val_loss: 5.2785e-04\n",
      "Epoch 158/200\n",
      "22220/22220 [==============================] - 5s 227us/step - loss: 7.9217e-04 - val_loss: 4.7241e-04\n",
      "Epoch 159/200\n",
      "22220/22220 [==============================] - 5s 220us/step - loss: 7.6638e-04 - val_loss: 4.7880e-04\n",
      "Epoch 160/200\n",
      "22220/22220 [==============================] - 3s 141us/step - loss: 7.4822e-04 - val_loss: 4.5895e-04\n",
      "Epoch 161/200\n",
      "22220/22220 [==============================] - 3s 138us/step - loss: 7.6879e-04 - val_loss: 4.5791e-04\n",
      "Epoch 162/200\n",
      "22220/22220 [==============================] - 3s 140us/step - loss: 7.6170e-04 - val_loss: 5.0546e-04\n",
      "Epoch 163/200\n",
      "22220/22220 [==============================] - 3s 142us/step - loss: 7.5499e-04 - val_loss: 4.6875e-04\n",
      "Epoch 164/200\n",
      "22220/22220 [==============================] - 3s 144us/step - loss: 7.5023e-04 - val_loss: 4.9345e-04\n",
      "Epoch 165/200\n",
      "22220/22220 [==============================] - 3s 143us/step - loss: 7.6667e-04 - val_loss: 4.9056e-04\n",
      "Epoch 166/200\n",
      "22220/22220 [==============================] - 3s 142us/step - loss: 7.5839e-04 - val_loss: 4.7566e-04\n",
      "Epoch 167/200\n",
      "22220/22220 [==============================] - 3s 141us/step - loss: 7.4621e-04 - val_loss: 4.6527e-04\n",
      "Epoch 168/200\n",
      "22220/22220 [==============================] - 3s 145us/step - loss: 7.2664e-04 - val_loss: 4.7107e-04\n",
      "Epoch 169/200\n",
      "22220/22220 [==============================] - 3s 145us/step - loss: 7.6275e-04 - val_loss: 4.8501e-04\n",
      "Epoch 170/200\n",
      "22220/22220 [==============================] - 3s 145us/step - loss: 7.5024e-04 - val_loss: 4.6333e-04\n",
      "Epoch 171/200\n",
      "22220/22220 [==============================] - 3s 144us/step - loss: 7.5715e-04 - val_loss: 4.5623e-04\n",
      "Epoch 172/200\n",
      "22220/22220 [==============================] - 3s 145us/step - loss: 7.3517e-04 - val_loss: 4.8072e-04\n",
      "Epoch 173/200\n",
      "22220/22220 [==============================] - 3s 144us/step - loss: 7.1568e-04 - val_loss: 5.0948e-04\n",
      "Epoch 174/200\n",
      "22220/22220 [==============================] - 3s 141us/step - loss: 7.3423e-04 - val_loss: 4.7185e-04\n",
      "Epoch 175/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 7.4477e-04 - val_loss: 4.5007e-04\n",
      "Epoch 176/200\n",
      "22220/22220 [==============================] - 3s 145us/step - loss: 7.0980e-04 - val_loss: 4.8579e-04\n",
      "Epoch 177/200\n",
      "22220/22220 [==============================] - 3s 144us/step - loss: 7.5575e-04 - val_loss: 4.7777e-04\n",
      "Epoch 178/200\n",
      "22220/22220 [==============================] - 3s 143us/step - loss: 7.3364e-04 - val_loss: 5.2807e-04\n",
      "Epoch 179/200\n",
      "22220/22220 [==============================] - 3s 144us/step - loss: 7.2613e-04 - val_loss: 5.1431e-04\n",
      "Epoch 180/200\n",
      "22220/22220 [==============================] - 3s 144us/step - loss: 7.2247e-04 - val_loss: 4.6275e-04\n",
      "Epoch 181/200\n",
      "22220/22220 [==============================] - 3s 150us/step - loss: 7.1332e-04 - val_loss: 4.8553e-04\n",
      "Epoch 182/200\n",
      "22220/22220 [==============================] - 5s 221us/step - loss: 7.0748e-04 - val_loss: 4.8448e-04\n",
      "Epoch 183/200\n",
      "22220/22220 [==============================] - 4s 200us/step - loss: 7.7570e-04 - val_loss: 5.1277e-04\n",
      "Epoch 184/200\n",
      "22220/22220 [==============================] - 3s 153us/step - loss: 7.0836e-04 - val_loss: 4.7568e-04\n",
      "Epoch 185/200\n",
      "22220/22220 [==============================] - 5s 224us/step - loss: 7.0518e-04 - val_loss: 4.9028e-04\n",
      "Epoch 186/200\n",
      "22220/22220 [==============================] - 3s 151us/step - loss: 7.0750e-04 - val_loss: 4.7659e-04\n",
      "Epoch 187/200\n",
      "22220/22220 [==============================] - 3s 139us/step - loss: 7.0797e-04 - val_loss: 4.6338e-04\n",
      "Epoch 188/200\n",
      "22220/22220 [==============================] - 3s 151us/step - loss: 7.3148e-04 - val_loss: 5.2069e-04\n",
      "Epoch 189/200\n",
      "22220/22220 [==============================] - 3s 146us/step - loss: 6.9719e-04 - val_loss: 4.8976e-04\n",
      "Epoch 190/200\n",
      "22220/22220 [==============================] - 3s 156us/step - loss: 6.8485e-04 - val_loss: 4.4676e-04\n",
      "Epoch 191/200\n",
      "22220/22220 [==============================] - 3s 142us/step - loss: 6.9369e-04 - val_loss: 4.7994e-04\n",
      "Epoch 192/200\n",
      "22220/22220 [==============================] - 3s 152us/step - loss: 7.1452e-04 - val_loss: 4.7462e-04\n",
      "Epoch 193/200\n",
      "22220/22220 [==============================] - 3s 140us/step - loss: 7.1373e-04 - val_loss: 5.0712e-04\n",
      "Epoch 194/200\n",
      "22220/22220 [==============================] - 4s 172us/step - loss: 6.8624e-04 - val_loss: 4.5698e-04\n",
      "Epoch 195/200\n",
      "22220/22220 [==============================] - 3s 154us/step - loss: 6.8016e-04 - val_loss: 4.5821e-04\n",
      "Epoch 196/200\n",
      "22220/22220 [==============================] - 3s 140us/step - loss: 7.6710e-04 - val_loss: 4.9032e-04\n",
      "Epoch 197/200\n",
      "22220/22220 [==============================] - 3s 143us/step - loss: 6.7389e-04 - val_loss: 4.6267e-04\n",
      "Epoch 198/200\n",
      "22220/22220 [==============================] - 3s 150us/step - loss: 6.9028e-04 - val_loss: 4.9853e-04\n",
      "Epoch 199/200\n",
      "22220/22220 [==============================] - 3s 142us/step - loss: 6.9263e-04 - val_loss: 4.2802e-04\n",
      "Epoch 200/200\n",
      "22220/22220 [==============================] - 3s 144us/step - loss: 6.7456e-04 - val_loss: 4.5302e-04\n",
      "6944/6944 [==============================] - 0s 44us/step\n"
     ]
    }
   ],
   "source": [
    "# choose a station number and train the model with the data for the given station\n",
    "station_number = 150\n",
    "result = pd.read_csv('./data/result.csv', encoding='utf-8')\n",
    "    \n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df = pd.read_csv('./data/departure/%d_2008_to_2017.csv' % station_number,encoding='utf-8', dtype='float64')\n",
    "data = np.concatenate(scaler.fit_transform(df.values.reshape(-1,1)))\n",
    "    \n",
    "SMAPE, RMSE, history = run_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the train and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06067401554908337"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
