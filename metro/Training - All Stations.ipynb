{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURS_PER_DAY = 20\n",
    "DAYS_PER_WEEK = 7\n",
    "DAYS_PER_YEAR = 365\n",
    "\n",
    "def create_dataset_years(signal_data, hours=1, days=1, weeks=1, years=1):\n",
    "    num_data = len(signal_data) - HOURS_PER_DAY * DAYS_PER_YEAR * years\n",
    "    x_arr, y_arr = np.zeros((num_data, 4, max(hours, days, weeks, years))), np.zeros((num_data,))\n",
    "    \n",
    "    for i in range(num_data):\n",
    "        index = i\n",
    "        \n",
    "        for j in range(years):\n",
    "            x_arr[i, 3, j] = signal_data[index]\n",
    "            index += HOURS_PER_DAY * DAYS_PER_YEAR\n",
    "            \n",
    "        index -= HOURS_PER_DAY * DAYS_PER_WEEK * weeks\n",
    "        \n",
    "        for j in range(weeks):\n",
    "            x_arr[i, 2, j] = signal_data[index]\n",
    "            index += HOURS_PER_DAY * DAYS_PER_WEEK\n",
    "            \n",
    "        index -= HOURS_PER_DAY * days\n",
    "        \n",
    "        for j in range(days):\n",
    "            x_arr[i, 1, j] = signal_data[index]\n",
    "            index += HOURS_PER_DAY\n",
    "        \n",
    "        x_arr[i, 0, 0:hours] = signal_data[(index-hours):index]\n",
    "        y_arr[i] = signal_data[index]\n",
    "\n",
    "    return x_arr, y_arr\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()  \n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(Dropout(0.2)) \n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "def run_model(data):\n",
    "    hours = 3\n",
    "    days = 4\n",
    "    weeks = 5\n",
    "    years = 5\n",
    "    batch_size = 256\n",
    "    \n",
    "    # create model\n",
    "    model = create_model()\n",
    "    adam = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "    \n",
    "    # prepare data\n",
    "    x_data, y_data = create_dataset_years(data, hours, days, weeks, years)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, shuffle=False)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # run model\n",
    "    history = model.fit(x_train, y_train, epochs=200, batch_size=batch_size, validation_data=(x_val, y_val))\n",
    "    score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "    # predict future values\n",
    "    predictions = np.concatenate(model.predict(x_test, batch_size))\n",
    "    #plt.plot(predictions, (predictions - y_test), 'rx')\n",
    "    \n",
    "    # evaluate model\n",
    "    SMAPE = np.mean(abs(predictions - y_test) / (abs(predictions) + abs(y_test)))\n",
    "    RMSE = np.sqrt(np.mean((predictions - y_test)**2))\n",
    "    \n",
    "    return SMAPE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('./data/result.csv', encoding='utf-8')\n",
    "station_numbers = result['station_number'].tolist()\n",
    "\n",
    "for i in range(len(station_numbers)):\n",
    "    \n",
    "    station_number = station_numbers[i]\n",
    "    result = pd.read_csv('./data/result.csv', encoding='utf-8')\n",
    "    \n",
    "    # skip stations already processed\n",
    "    if result[result['station_number'] == station_number].at[i, 'SMAPE'] != 0.0:\n",
    "        continue\n",
    "        \n",
    "    print(\"Now processing station number %d\" % station_number)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df = pd.read_csv('./data/departure/%d_2008_to_2017.csv' % station_number,encoding='utf-8', dtype='float64')\n",
    "    data = np.concatenate(scaler.fit_transform(df.values.reshape(-1,1))) \n",
    "    \n",
    "    SMAPE, RMSE = run_model(data)\n",
    "    \n",
    "    result.loc[result.station_number == station_number, 'SMAPE'] = SMAPE\n",
    "    result.loc[result.station_number == station_number, 'RMSE'] = RMSE\n",
    "    \n",
    "    result.to_csv('./data/result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
